#!/usr/bin/env python3

## Note: Within this script, a "path" is always absolute and `cat`-able.  Eg, s3 paths are `/mnt/s3/path/to/file`, not `s3://`.
##       That can be surprising, because this handles all s3 buckets you can access, including buckets that aren't mounted in /mnt/s3/.
##       But I prefer `/mnt/s3/` over `s3://`.

# TODO: Support `v /path/to/chr#range#-#.tsv`.  If multiple files match, list them.  If only one, show it.  #=\d+  %=.*

# TODO: Add custom bash_completion using <https://github.com/kislyuk/argcomplete>

# TODO: Add `--less` which does `less -S`

import sys, os, functools, json, datetime, itertools, re, shutil, io, argparse, textwrap, math
from pprint import pprint
from collections import Counter
import signal; signal.signal(signal.SIGPIPE, signal.SIG_DFL)
from typing import List,Optional,Dict

term_width = shutil.get_terminal_size().columns

OPEN_IN_IPYTHON = False  # This is easier than passing args down thru functions
OPEN_IN_VISIDATA = False
MAX_NUM_LINES = None

def run(argv: List[str] = None):
    if argv is None: argv = sys.argv
    args = parse_args(argv)
    global OPEN_IN_IPYTHON, OPEN_IN_VISIDATA, MAX_NUM_LINES
    if args.py: OPEN_IN_IPYTHON = True
    if args.vd: OPEN_IN_VISIDATA = True
    if args.num_lines: MAX_NUM_LINES = args.num_lines
    if args.all: MAX_NUM_LINES = sys.maxsize
    if not args.path and not sys.stdin.isatty(): show_lines(read_stdin_lines_and_switch_to_tty())
    else: show_path(args.path or '.')

def parse_args(argv: List[str] = None) -> argparse.Namespace:
    if argv is None: argv = sys.argv
    parser = argparse.ArgumentParser(
        prog='v',
        usage=textwrap.dedent('''
           $ v s3://rgc-ag-data/app_data/finemap/chip_UKB_GHS/CHIP_
            /mnt/s3/rgc-ag-data/app_data/finemap/chip_UKB_GHS/CHIP_UKB_GHS.regions
            /mnt/s3/rgc-ag-data/app_data/finemap/chip_UKB_GHS/CHIP_UKB_GHS.chr#_range#-#.config    (26, eg: 8_range128813195-130313195)
            /mnt/s3/rgc-ag-data/app_data/finemap/chip_UKB_GHS/CHIP_UKB_GHS.chr#_range#-#.cred#     (62, eg: 8_range128813195-130313195.cred3)
            /mnt/s3/rgc-ag-data/app_data/finemap/chip_UKB_GHS/CHIP_UKB_GHS.chr#_range#-#.ld.gz     (26, eg: 8_range128813195-130313195)
            /mnt/s3/rgc-ag-data/app_data/finemap/chip_UKB_GHS/CHIP_UKB_GHS.chr#_range#-#.log_sss   (26, eg: 8_range128813195-130313195)
            /mnt/s3/rgc-ag-data/app_data/finemap/chip_UKB_GHS/CHIP_UKB_GHS.chr#_range#-#.snp       (26, eg: 8_range128813195-130313195)
            /mnt/s3/rgc-ag-data/app_data/finemap/chip_UKB_GHS/CHIP_UKB_GHS.chr#_range#-#.snp_cred  (26, eg: 8_range128813195-130313195)

           $ v /mnt/s3/rgc-ag-data/app_data/finemap/UKB_HAL_NMSC/UKB_GHS_NMSC_HAL.chr9_range518755-761780.SuSiE.tsv
            => Columns:
              - CONST       Region        'chr9_range518755-761780'
              - uniq        rsid          '9:518755:G:A' - '9:637391:A:C'
              - CONST int   chromosome    9
              - uniq  int   position      518,755 - 637,391
              -             allele1       counts={277*'G' 274*'C' 241*'A' 173*'T'}
              -             allele2       counts={287*'G' 235*'A' 235*'T' 218*'C'}
              -       float maf           0.001 - 0.499
              -       float beta          -0.381 - 0.256
              -       float se            0.00802 - 0.164
              - CONST       status        'success'
              -       float PIP           0.0128 - 0.01944
              - CONST float CS1(median|ld|=0.900834157239296)   nan

            => Head:
              rsid          position allele allele  maf        beta      se        PIP
             9:518755:G:A   518755   G      A      0.4970     0.00011   0.00842   0.0128
             9:518898:T:G   518898   T      G      0.0014    -0.01576   0.14180   0.0128
             9:518908:G:A   518908   G      A      0.3601     0.01821   0.00856   0.0147
             9:519047:G:C   519047   G      C      0.2854     0.01392   0.00908   0.0147
             9:519074:G:GA  519074   G      GA     0.0012    -0.01138   0.11820   0.0127
             9:519182:G:A   519182   G      A      0.2218     0.01779   0.00985   0.0149
             9:519210:A:G   519210   A      G      0.0529    -0.01184   0.01808   0.0128
             9:519339:A:T   519339   A      T      0.0034    -0.09898   0.06853   0.0134
             9:519594:C:G   519594   C      G      0.0051     0.03430   0.05666   0.0127

        ---
        '''),
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument('-a', '--all', action='store_true', help='Output all lines, without truncating or summarizing to fit your terminal.  Sets `-n` to infinity.`')
    parser.add_argument('-n', dest='num_lines', type=int, default=None, help='Maximum number of lines to show from file or pathlist.')
    parser.add_argument('--py', action='store_true', help='Open the result in IPython.')
    parser.add_argument('--vd', action='store_true', help='Open the result in VisiData.')
    parser.add_argument('path', nargs='?', help=argparse.SUPPRESS)
    return parser.parse_args()



def show_path(path:str):
    path = get_absolute_path(path); assert not path.endswith('/')
    if path == '/mnt/s3': [print(f' - /mnt/s3/{bucket}/') for bucket in s3_list_all_buckets()]
    elif path.startswith('/mnt/s3/'): show_s3_path(path)
    elif os.path.isfile(path): show_file(path)
    elif os.path.isdir(path): show_dir(path)
    else: show_completions(path)

def show_s3_path(path:str):
    bucket_and_key = remove_required_prefix(path, '/mnt/s3/')
    bucket, key = bucket_and_key.split('/', 1) if '/' in bucket_and_key else (bucket_and_key, '')
    if s3_exists(bucket, key): show_s3_file(bucket, key)
    elif s3_exists(bucket, key+'/'): show_s3_dir(bucket, key+'/')
    else: show_s3_completions(bucket, key)

def show_s3_file(bucket:str, key:str):
    print(f'=> File /mnt/s3/{bucket}/{key}')
    mtime_str = s3_get_object(bucket, key)['LastModified'].strftime('%Y-%m-%d_%H:%M:%S')
    length = int(s3_get_object(bucket, key)['ContentLength'])
    if length > 1e9: length_str = f'{int(length/1e9)} GB'
    elif length > 1e6: length_str = f'{int(length/1e6)} MB'
    elif length > 1e3: length_str = f'{int(length/1e3)} KB'
    else: length_str = f'{length} B'
    print(f'   {length_str}      {mtime_str}')
    #pprint(s3_get_object(bucket, key))
    if length:
        show_file(f'/mnt/s3/{bucket}/{key}')

def show_s3_dir(bucket:str, key:str):
    print(f'=> Dir /mnt/s3/{bucket}/{key}')
    s3_obj = s3_get_object_if_exists(bucket, key)  # Some dirs don't exist as s3 keys!
    if s3_obj: print(' => mtime =', datetime.datetime.fromtimestamp(int(s3_obj['Metadata']['mtime'])).strftime('%Y-%m-%d_%H:%M:%S'))
    show_pathlist(s3_list_dir(bucket, key, output_trailing_slash=True), shared_prefix=f'/mnt/s3/{bucket}/{key}')

def show_s3_completions(bucket:str, key:str):
    print(f'=> Completions of /mnt/s3/{bucket}/{key}')
    show_pathlist(s3_list_dir(bucket, key, output_trailing_slash=True, partial_filename=True), shared_prefix=f'/mnt/s3/{bucket}/{key}')

def show_file(path:str):
    try:
        with smarter_open(path, 'rt') as f:
            lines = list(itertools.islice(f, 0, MAX_NUM_LINES or 1000))  # TODO: Read the first 1MB?
    except UnicodeDecodeError:
        with smarter_open(path, 'rb') as f:
            print(repr(f.read(500)))
    else:
        show_lines(lines)

def show_dir(path:str):
    show_pathlist([f'{path}/{name}' for name in os.listdir(path)], shared_prefix=path)

def show_completions(path:str):
    dir_path, basename = os.path.split(path)
    show_pathlist([f'{dir_path}/{name}' for name in os.listdir(dir_path) if name.startswith(basename)], shared_prefix=path)



## Utils:

def show_lines(lines:List[str]):
    ## Try to make a table:
    for delim in '\t, ;|':
        # TODO: Find cols that are all int/float, and use them to determine what's data, what's colnames, and what's metadata
        # TODO: Guess whether lines[0] is a header or just a row.  If it has int everywhere lines[1] does, it's a row.
        # TODO: Handle quotes?
        ## Ignore # lines when detecting a table
        ## Then add # lines back into the table if they have num_fields
        hash_lines = []
        for line in lines:
            if not line.startswith('#'): break
            hash_lines.append(line)
        nohash_lines = lines[len(hash_lines):]
        if delim in nohash_lines[0]:
            num_fields = nohash_lines[0].count(delim)
            if all(line.count(delim) == num_fields for line in nohash_lines):
                good_hash_lines, bad_hash_lines = [], []
                for line in hash_lines:
                    if line.count(delim) == num_fields: good_hash_lines.append(line)
                    else: bad_hash_lines.append(line)
                for line in bad_hash_lines: print(line)
                table = [line.rstrip('\n').split(delim) for line in good_hash_lines+nohash_lines]
                show_table(table)
                return
    ## Give up and just print:
    # TODO: Say if truncated
    for line in lines[:MAX_NUM_LINES or 30]: print(line.rstrip('\n'))

def show_table(table:List[List[str]]):
    import numpy as np
    if table[0][0].startswith('\ufeff'): table[0][0] = table[0][0][1:]
    if OPEN_IN_IPYTHON:
        import IPython
        df = get_df(table)
        print('>>> df'); print(df); print()
        IPython.start_ipython(argv=['--no-confirm-exit'], display_banner=False, user_ns={'df':df})
    elif OPEN_IN_VISIDATA or (MAX_NUM_LINES and len(table)>40):
        import visidata
        print('VisiData sheet commands:  C=summarize_columns.  q=close.')
        print('VisiData column commands:  F=tabulate.  ]=sort.  -=hide.  !=favorite.')
        print('VisiData row commands:  enter=focus_row.  d=delete.  gz^=replace_headers.')
        df = get_df(table)
        visidata.vd.view_pandas(df)
    else:
        ## Truncate if too wide
        if len(table[0]) > 100:
            colnames = table[0]
            print(f'   {len(table):,} rows * {len(colnames):,} columns.  Truncating.')
            # Try deleting numbers to see if it gives a reasonable number of patterns to show:
            numless_colname_counts = Counter(re.subn(r'[0-9]+', '#', c)[0] for c in colnames)
            numless_colnames = {re.subn(r'[0-9]+', '#', c)[0]:c for c in colnames}
            if len(numless_colnames) < 200:
                print('=> Too many columns to show, but they match these patterns:')
                # TODO: Indent this:
                print('  ', [numless_colnames[nc] if count==1 else nc for nc,count in numless_colname_counts.items()])
            table = [row[:15] for row in table]
        print()

        # Gather info about columns
        colnames = table[0]
        num_cols = len(colnames)
        p80_colname_len = sorted(len(colname) for colname in colnames)[int(num_cols*.8)]
        df = get_df(table)
        info_for_colidx = {}
        for colidx, colname in enumerate(table[0]):
            info = info_for_colidx.setdefault(colidx, {})
            info['colname'] = colname
            info['series'] = df[colname]
            # Check counts:
            counter = Counter(row[colidx] for row in table[1:])
            info['counter'] = counter
            info['biggest_count'] = counter.most_common(1)[0][1]
            info['is_const'] = len(counter) == 1
            info['is_uniq'] = len(counter) == len(table[1:])
            dtype = str(df[colname].dtype)
            info['type'] = {'object':'', 'float64':'float', 'int64':'int'}.get(dtype, dtype)

        ## Summarize columns:
        print('=> Columns:')
        for colidx, info in info_for_colidx.items():
            is_const, is_uniq, coltype, colname, series, biggest_count, counter = info['is_const'], info['is_uniq'], info['type'], info['colname'], info['series'], info['biggest_count'], info['counter']
            const_uniq_str = 'CONST' if is_const else ('uniq' if is_uniq else '')
            line = f'  - {const_uniq_str:5} {coltype:5} {colname:{p80_colname_len+3}}   '
            if is_const:
                line += f'{short_repr(series.iloc[1], 60)}'
            elif coltype == 'float' and biggest_count < 5:
                min_val, max_val = series.min(), series.max()
                # TODO: Round 0.1001 to 0.100 not 0.1
                difference_decimal_places = math.ceil(-np.log10(max_val - min_val) + 2)
                min_val_decimal_places = math.ceil(-np.log10(np.abs(min_val)) + 2) if min_val>0 else 0
                max_val_decimal_places = math.ceil(-np.log10(np.abs(max_val)) + 2) if max_val<0 else 0
                min_val = np.round(min_val, max(1, difference_decimal_places, min_val_decimal_places))
                max_val = np.round(max_val, max(1, difference_decimal_places, max_val_decimal_places))
                line += f'{min_val} - {max_val}'
            elif coltype == 'int' and biggest_count < 5:
                min_val, max_val = series.min(), series.max()
                line += f'{repr_int(min_val)} - {repr_int(max_val)}'
            elif is_uniq:
                line += f'{short_repr(series.min(), 30)} - {short_repr(series.max(), 30)}'
            elif len(counter) < 7:
                line += 'counts={' + ' '.join(f'{count}*{short_repr(elem, 20)}' for elem,count in counter.most_common()) + '}'
            else:
                line += 'counts={' + ' '.join(f'{count}*{short_repr(elem, 20)}' for elem,count in counter.most_common(4)) + ' …}'
            print(line)
        print()

        ## Show table head:
        # TODO: Align decimals of floats.
        # TODO: Align ints to the right
        if not MAX_NUM_LINES and len(table)>20:
            print('=> Head:')
            table = table[:10]
        for colidx,info in info_for_colidx.items():
            cell_widths = sorted(len(row[colidx]) for row in table)
            info['colwidth'] = max(4, cell_widths[len(table)*9//10]) + 2
            if info['is_const']: info['colwidth'] = 0
        def get_total_width(): return sum(info['colwidth']+1 for info in info_for_colidx.values() if info['colwidth'])
        if get_total_width() >= term_width - 3:
            for info in info_for_colidx.values():
                if info['colwidth'] > 20: info['colwidth'] = 15
        if get_total_width() >= term_width - 3:
            for num_cols_that_fit in range(1, 1+num_cols):
                total_width = sum(1+info_for_colidx[colidx]['colwidth'] for colidx in range(num_cols_that_fit))
                if total_width > term_width: num_cols_that_fit -= 1; break
            if num_cols_that_fit > 0:
                table = [row[:num_cols_that_fit] for row in table]
            else:
                print('=> Trucated wide table:')
                info_for_colidx[0]['colwidth'] = term_width-4
                table = [[row[0][:term_width-4]] for row in table]
        for rowidx,row in enumerate(table):
            for colidx, cell in enumerate(row):
                info = info_for_colidx[colidx]
                width = info['colwidth']
                if width == 0: continue
                r = f'{cell[:width]:{width}} '
                if rowidx == 0 and len(table)>1 and r.endswith('  ') and len(r.rstrip()) < len(str(table[1][colidx])):
                    r = ' ' + r[:-1]
                elif r.startswith('0.0') or r.startswith('-0.0'):
                    nonzero = r.lstrip('-0.')
                    zero = r[:-len(nonzero)]
                    r = '\x1B[1;32m' + zero + '\x1B[0m' + nonzero
                print(r, end='')
            print()

def short_repr(x, maxwidth=20) -> str:
    r = repr(x)
    if len(r) <= maxwidth: return r
    sidewidth = 1+maxwidth//4
    return f'{r[:sidewidth]}…{r[-sidewidth:]}(len={len(r)})'

def repr_int(x) -> str:
    return str(x) if abs(x) < 10e3 else f'{x:,}'

def get_df(table:List[List[str]]) -> "pd.DataFrame":
    import pandas as pd
    df = pd.DataFrame(table[1:], columns=table[0])
    return pd.read_csv(io.StringIO(df.to_csv(index=False)))  # TODO: Don't make two dataframes!

def show_pathlist(pathlist:List[str], shared_prefix:str = ''):
    if not pathlist: print(' - Nothing matched!'); return
    suffixes = [remove_required_prefix(path, shared_prefix) for path in pathlist]
    assert len(suffixes) == len(set(suffixes))
    if MAX_NUM_LINES:
        if len(suffixes) <= MAX_NUM_LINES:
            for suf in suffixes: print(f' - {deemphasize(shared_prefix)}{suf}')
        else:
            for suf in suffixes[:MAX_NUM_LINES]: print(f' - {deemphasize(shared_prefix)}{suf}')
            print(f' => {len(suffixes)} total')
    else:
        if len(suffixes) < 50:
            for suf in suffixes: print(f' - {deemphasize(shared_prefix)}{suf}')
        else:
            ## Try replacing numbers with #.
            numless_suffix_counts = Counter(re.subn(r'[0-9]+', '#', suf)[0] for suf in suffixes)
            numless_suffixes = {re.subn(r'[0-9]+', '#', suf)[0]:suf for suf in suffixes}
            if len(numless_suffixes) < 40 and len(numless_suffixes) < len(suffixes)/2:
                for nsuf,suf in numless_suffixes.items():
                    if nsuf != suf:
                        example = re.search(r'[0-9].*[0-9]', suf).group(0)
                        spaces = ' '*(8 - (len(nsuf) % 4))  # Align to a grid, to hopefully align some lines
                        print(f' - {deemphasize(shared_prefix)}{nsuf}{spaces}({numless_suffix_counts[nsuf]}, eg: {example})')
                    else:
                        print(f' - {deemphasize(shared_prefix)}{suf}')
            else:
                # TODO: Handle `*.gz(.tbi)?`
                #       eg, /mnt/s3/rgc-ag-data/app_data/pheweb/UKB/UKB_Freeze_450/ALL/generated-by-pheweb/pheno_gz/
                #       Just try collapsing each file into its prefix.  If that cuts dirsize by >40%, show it.
                for suf in suffixes[:20]: print(f' - {deemphasize(shared_prefix)}{suf}')
                print(f' => {len(suffixes)} total')
    if OPEN_IN_IPYTHON:
        print('=> Saved to variable `pathlist`.\n')
        import IPython
        IPython.start_ipython(argv=['--no-confirm-exit'], display_banner=False, user_ns={'pathlist':pathlist})  # TODO: import os, pathlib, etc

def emphasize(text:str) -> str:
    return '\x1B[1;34m' + text + '\x1B[0m'
def deemphasize(text:str) -> str:
    return '\x1B[1;32m' + text + '\x1B[0m'

def get_absolute_path(path:str):
    '''Like os.path.abspath(), but handles `s3://`'''
    ## TODO: support dx_proj:/dx_path
    if path.startswith('s3://'): path = '/mnt/s3/' + removeprefix(path, 's3://')
    path = os.path.abspath(path)  # Note: This strips trailing /.
    assert path.startswith('/')
    assert not path.endswith('/')
    return path

def s3_exists(bucket:str, key:str) -> bool:
    import boto3, botocore
    try: s3_get_object(bucket, key); return True
    except botocore.exceptions.ClientError: pass
    if key.endswith('/'):
        # Note: Sometimes a directory doesn't exist on s3 as a key.  But it still has contents.
        #       Eg, s3://rgc-ag-data/app_data/pheweb/UKB/UKB_Freeze_450/ALL/generated-by-pheweb/
        return get_s3().list_objects_v2(Bucket=bucket, Prefix=key, MaxKeys=1)['KeyCount'] > 0
    return False

def s3_list_dir(bucket:str, key:str, output_trailing_slash=False, partial_filename=False) -> List[str]:
    '''
    Returns ["{bucket}/{key}", ...].
    If `output_trailing_slash`, then directories will have a trailing slash.
    That trailing slash is needed for `s3.get_object()`.
    But a normal filesystem doesn't include trailing slashes, so it's off by default.
    If `partial_filename`, you can use enter a prefix to get completions, like `s3_list_dir('s3://rgc-ag-data/a')`.
    '''
    import botocore
    if not partial_filename and key and not key.endswith('/'): key += '/'
    result_keys = []
    try:
        for page in s3_paginate('list_objects_v2', Bucket=bucket, Prefix=key, Delimiter='/'):
            for x in page.get('CommonPrefixes',[]):
                result_keys.append(x['Prefix'])
            for x in page.get('Contents',[]):
                if x['Key'] != key:
                    result_keys.append(x['Key'])
    except botocore.exceptions.ClientError as exc:
        raise Exception(f"Failed to get s3://{bucket}/{key}") from exc
    if not output_trailing_slash:
        result_keys = [k.rstrip('/') for k in result_keys]
    return [f'/mnt/s3/{bucket}/{k}' for k in result_keys]

def s3_paginate(s3_command:str, **kwargs):
    paginator = get_s3().get_paginator(s3_command)
    return paginator.paginate(**kwargs)

def s3_list_all_buckets() -> List[str]:
    return [bucket['Name'] for bucket in get_s3().list_buckets()['Buckets']]

def s3_get_head(bucket:str, key:str, bytes:int = 500) -> bytes:
    import botocore
    try:
        return get_s3().get_object(Bucket=bucket, Key=key, Range=f'bytes=0-{bytes}')['Body'].read()
    except botocore.exceptions.ClientError as exc:
        raise Exception(f"Failed to get s3://{bucket}/{key}") from exc

def s3_get_object_if_exists(bucket:str, key:str) -> Optional[dict]:
    import botocore
    try: return s3_get_object(bucket, key)
    except botocore.exceptions.ClientError: return None
@functools.lru_cache(None)
def s3_get_object(bucket:str, key:str) -> dict:
    return get_s3().get_object(Bucket=bucket, Key=key)

@functools.lru_cache(None)
def get_s3() -> "boto3.client":
    import boto3
    return boto3.client('s3')

def smarter_open(path:str, mode:str = 'r'):
    import smart_open
    if path.startswith('/mnt/s3/'):
        path = 's3://' + remove_required_prefix(path, '/mnt/s3/')
    return smart_open.smart_open(path, mode)

def removeprefix(s:str, prefix:str) -> str:
    '''Backport of str.removeprefix(prefix)'''
    if s.startswith(prefix):
        return s[len(prefix):]
    return s

def remove_required_prefix(s:str, prefix:str) -> str:
    assert s.startswith(prefix)
    return s[len(prefix):]

def read_stdin_lines_and_switch_to_tty() -> List[str]:
    data = sys.stdin.readlines()
    #data = os.fdopen(os.dup(0), 'rb')
    os.dup2(os.open("/dev/tty", os.O_RDONLY), 0)
    return data



if __name__ == '__main__':
    run()
