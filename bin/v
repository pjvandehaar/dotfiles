#!/usr/bin/env python3

## Note: Within this script, a "path" is always absolute and `cat`-able.  Eg, s3 paths are `/mnt/s3/path/to/file`, not `s3://`.
##       That can be surprising, because this handles all s3 buckets you can access, including buckets that aren't mounted in /mnt/s3/.
##       But I prefer `/mnt/s3/` over `s3://`.

# TODO: Support `v /path/to/chr#range#-#.tsv`.  If multiple files match, list them.  If only one, show it.  #=\d+  %=.*

# TODO: Add custom bash_completion using <https://github.com/kislyuk/argcomplete>

import sys, os, functools, json, datetime, itertools, re, shutil, io, argparse, textwrap
from pprint import pprint
from collections import Counter
import signal; signal.signal(signal.SIGPIPE, signal.SIG_DFL)
from typing import List

term_width = shutil.get_terminal_size().columns

OPEN_IN_IPYTHON = False  # This is easier than passing args down thru functions
OPEN_IN_VISIDATA = False
MAX_NUM_LINES = None

def run(argv: List[str] = None):
    if argv is None: argv = sys.argv
    args = parse_args(argv)
    global OPEN_IN_IPYTHON, OPEN_IN_VISIDATA, MAX_NUM_LINES
    if args.py: OPEN_IN_IPYTHON = True
    if args.vd: OPEN_IN_VISIDATA = True
    if args.num_lines: MAX_NUM_LINES = args.num_lines
    if args.all: MAX_NUM_LINES = sys.maxsize
    if not args.path and not sys.stdin.isatty(): show_lines(read_stdin_lines_and_switch_to_tty())
    else: show_path(args.path or '.')

def parse_args(argv: List[str] = None) -> argparse.Namespace:
    if argv is None: argv = sys.argv
    parser = argparse.ArgumentParser(
        prog='v',
        usage=textwrap.dedent('''
          v /path/to/dir/         Lists files, like `ls`
          v /path/to/d            Shows completions, like `ls /path/to/d*`
          v /path/to/file         Shows the file, with a spreadsheet for tabular data
          cat file | v            Same as `v file`

        This script handles gzip and s3://
        '''),
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument('-a', '--all', action='store_true', help='Output all lines, without truncating or summarizing to fit your terminal.  Sets `-n` to infinity.`')
    parser.add_argument('-n', '--num-lines', type=int, default=None, help='Maximum number of lines to show from file or pathlist.')
    parser.add_argument('--py', action='store_true', help='Open the result in IPython.')
    parser.add_argument('--vd', action='store_true', help='Open the result in VisiData.')
    parser.add_argument('path', nargs='?', help=argparse.SUPPRESS)
    return parser.parse_args()



def show_path(path:str):
    path = get_absolute_path(path); assert not path.endswith('/')
    if path == '/mnt/s3': [print(f' - /mnt/s3/{bucket}/') for bucket in s3_list_all_buckets()]
    elif path.startswith('/mnt/s3/'): show_s3_path(path)
    elif os.path.isfile(path): show_file(path)
    elif os.path.isdir(path): show_dir(path)
    else: show_completions(path)

def show_s3_path(path:str):
    bucket_and_key = remove_required_prefix(path, '/mnt/s3/')
    bucket, key = bucket_and_key.split('/', 1) if '/' in bucket_and_key else (bucket_and_key, '')
    if s3_exists(bucket, key): show_s3_file(bucket, key)
    elif s3_exists(bucket, key+'/'): show_s3_dir(bucket, key+'/')
    else: show_s3_completions(bucket, key)

def show_s3_file(bucket:str, key:str):
    print(f'=> File /mnt/s3/{bucket}/{key}')
    mtime_str = s3_get_object(bucket, key)['LastModified'].strftime('%Y-%m-%d_%H:%M:%S')
    length = int(s3_get_object(bucket, key)['ContentLength'])
    if length > 1e9: length_str = f'{int(length/1e9)} GB'
    elif length > 1e6: length_str = f'{int(length/1e6)} MB'
    elif length > 1e3: length_str = f'{int(length/1e3)} KB'
    else: length_str = f'{length} B'
    print(f'   {length_str}      {mtime_str}')
    #pprint(s3_get_object(bucket, key))
    if length:
        show_file(f'/mnt/s3/{bucket}/{key}')

def show_s3_dir(bucket:str, key:str):
    print(f'=> Dir /mnt/s3/{bucket}/{key}')
    print(' => mtime =', datetime.datetime.fromtimestamp(int(s3_get_object(bucket, key)['Metadata']['mtime'])).strftime('%Y-%m-%d_%H:%M:%S'))
    show_pathlist(s3_list_dir(bucket, key, output_trailing_slash=True), shared_prefix=f'/mnt/s3/{bucket}/{key}')

def show_s3_completions(bucket:str, key:str):
    print(f'=> Completions of /mnt/s3/{bucket}/{key}')
    show_pathlist(s3_list_dir(bucket, key, output_trailing_slash=True, partial_filename=True), shared_prefix=f'/mnt/s3/{bucket}/{key}')

def show_file(path:str):
    try:
        with smarter_open(path, 'rt') as f:
            lines = list(itertools.islice(f, 0, MAX_NUM_LINES or 1000))
    except UnicodeDecodeError:
        with smarter_open(path, 'rb') as f:
            print(repr(f.read(500)))
    else:
        show_lines(lines)

def show_dir(path:str):
    show_pathlist([f'{path}/{name}' for name in os.listdir(path)], shared_prefix=path)

def show_completions(path:str):
    dir_path, basename = os.path.split(path)
    show_pathlist([f'{dir_path}/{name}' for name in os.listdir(dir_path) if name.startswith(basename)], shared_prefix=path)



## Utils:

def show_lines(lines:List[str]):
    ## Try to make a table:
    for delim in '\t, ;|':
        # TODO: Find cols that are all int/float, and use them to determine what's data, what's colnames, and what's metadata
        # TODO: Guess whether lines[0] is a header or just a row
        # TODO: Handle quotes?
        ## Ignore # lines when detecting a table
        ## Then add # lines back into the table if they have num_fields
        hash_lines = []
        for line in lines:
            if not line.startswith('#'): break
            hash_lines.append(line)
        nohash_lines = lines[len(hash_lines):]
        if delim in nohash_lines[0]:
            num_fields = nohash_lines[0].count(delim)
            if all(line.count(delim) == num_fields for line in nohash_lines):
                good_hash_lines, bad_hash_lines = [], []
                for line in hash_lines:
                    if line.count(delim) == num_fields: good_hash_lines.append(line)
                    else: bad_hash_lines.append(line)
                for line in bad_hash_lines: print(line)
                table = [line.rstrip('\n').split(delim) for line in good_hash_lines+nohash_lines]
                show_table(table)
                return
    ## Give up and just print:
    # TODO: Say if truncated
    for line in lines[:MAX_NUM_LINES or 30]: print(line.rstrip('\n'))

def show_table(table:List[List[str]]):
    if OPEN_IN_IPYTHON:
        import IPython
        df = get_df(table)
        print('>>> df'); print(df); print()
        IPython.start_ipython(argv=['--no-confirm-exit'], display_banner=False, user_ns={'df':df})
    else:
        column_widths = [0]*len(table[0])
        for colidx in range(len(table[0])):
            cell_widths = sorted(len(row[colidx]) for row in table)
            column_widths[colidx] = max(4, cell_widths[len(table)*9//10]) + 2
        if OPEN_IN_VISIDATA or (MAX_NUM_LINES and len(table)>40):
            print('VisiData sheet commands:  C=summarize_columns.  q=close.')
            print('VisiData column commands:  F=tabulate.  ]=sort.  -=hide.  !=favorite.')
            print('VisiData row commands:  enter=focus_row.  d=delete.  gz^=replace_headers.')
            import visidata
            df = get_df(table)
            visidata.vd.view_pandas(df)
        else:
            ## Summarize columns:
            df = get_df(table)
            print('=> Columns:')
            for colidx, colname in enumerate(table[0]):
                coltype = str(df[colname].dtype)
                coltype = {'object':'str', 'float64':'flt', 'int64':'int'}.get(coltype, coltype)
                counter = Counter(row[colidx] for row in table[1:])
                line = f'  - [{coltype}] {colname}    '
                if len(counter) == 1: line += f'  CONST({table[1][colidx]})'
                elif len(counter) == len(table): line += '  UNIQUE'
                else:
                    if coltype in ['int','flt'] and counter.most_common(1)[0][1] < 5:
                        line += f'  min={df[colname].min()}  max={df[colname].max()}'
                    else:
                        #line += f'  counts={dict(counter.most_common(3))}'
                        line += '  counts={' + ' '.join(f'{count}*{repr(elem)}' for elem,count in counter.most_common(3)) + '}'
                print(line)
            ## Show table head:
            print()
            # TODO: Align decimals of floats
            # TODO: Align ints to the right, with thousands commas
            # TODO: Center colnames
            if not MAX_NUM_LINES: table = table[:10]
            for num_cols_that_fit in range(1, 1+len(table[0])):
                total_width = sum(1+column_widths[colidx] for colidx in range(num_cols_that_fit))
                if total_width > term_width: num_cols_that_fit -= 1; break
            for rowidx,row in enumerate(table):
                for colidx, cell in enumerate(row[:num_cols_that_fit]):
                    width = column_widths[colidx]
                    r = f'{cell[:width]:{width}} '
                    if r.startswith('0.0') or r.startswith('-0.0'):
                        nonzero = r.lstrip('-0.')
                        zero = r[:-len(nonzero)]
                        r = '\x1B[1;32m' + zero + '\x1B[0m' + nonzero
                    print(r, end='')
                print()

def get_df(table:List[List[str]]) -> "pd.DataFrame":
    import pandas as pd
    df = pd.DataFrame(table[1:], columns=table[0])
    return pd.read_csv(io.StringIO(df.to_csv(index=False)))  # TODO: Don't make two dataframes!

def show_pathlist(pathlist:List[str], shared_prefix:str = ''):
    if not pathlist: print(' - Nothing matched!'); return
    suffixes = [remove_required_prefix(path, shared_prefix) for path in pathlist]
    assert len(suffixes) == len(set(suffixes))
    if MAX_NUM_LINES:
        if len(suffixes) <= MAX_NUM_LINES:
            for suf in suffixes: print(f' - {deemphasize(shared_prefix)}{suf}')
        else:
            for suf in suffixes[:MAX_NUM_LINES]: print(f' - {deemphasize(shared_prefix)}{suf}')
            print(f' => {len(suffixes)} total')
    else:
        if len(suffixes) < 50:
            for suf in suffixes: print(f' - {deemphasize(shared_prefix)}{suf}')
        else:
            ## Try replacing numbers with #.
            numless_suffix_counts = Counter(re.subn(r'[0-9]+', '#', suf)[0] for suf in suffixes)
            numless_suffixes = {re.subn(r'[0-9]+', '#', suf)[0]:suf for suf in suffixes}
            if len(numless_suffixes) < 40 and len(numless_suffixes) < len(suffixes)/2:
                for nsuf,suf in numless_suffixes.items():
                    if nsuf != suf:
                        example = re.search(r'[0-9].*[0-9]', suf).group(0)
                        spaces = ' '*(8 - (len(nsuf) % 4))  # Align to a grid, to hopefully align some lines
                        print(f' - {deemphasize(shared_prefix)}{nsuf}{spaces}({numless_suffix_counts[nsuf]}, eg: {example})')
                    else:
                        print(f' - {deemphasize(shared_prefix)}{suf}')
            else:
                for suf in suffixes[:20]: print(f' - {deemphasize(shared_prefix)}{suf}')
                print(f' => {len(suffixes)} total')
    if OPEN_IN_IPYTHON:
        print('=> Saved to variable `pathlist`.\n')
        import IPython
        IPython.start_ipython(argv=['--no-confirm-exit'], display_banner=False, user_ns={'pathlist':pathlist})  # TODO: import os, pathlib, etc

def emphasize(text:str) -> str:
    return '\x1B[1;34m' + text + '\x1B[0m'
def deemphasize(text:str) -> str:
    return '\x1B[1;32m' + text + '\x1B[0m'

def get_absolute_path(path:str):
    '''Like os.path.abspath(), but handles `s3://`'''
    ## TODO: support dx_proj:/dx_path
    if path.startswith('s3://'): path = '/mnt/s3/' + removeprefix(path, 's3://')
    path = os.path.abspath(path)  # Note: This strips trailing /.
    assert path.startswith('/')
    assert not path.endswith('/')
    return path

def s3_exists(bucket:str, key:str) -> bool:
    import boto3, botocore
    try: s3_get_object(bucket, key); return True
    except botocore.exceptions.ClientError: return False

def s3_list_dir(bucket:str, key:str, output_trailing_slash=False, partial_filename=False) -> List[str]:
    '''
    Returns ["{bucket}/{key}", ...].
    If `output_trailing_slash`, then directories will have a trailing slash.
    That trailing slash is needed for `s3.get_object()`.
    But a normal filesystem doesn't include trailing slashes, so it's off by default.
    If `partial_filename`, you can use enter a prefix to get completions, like `s3_list_dir('s3://rgc-ag-data/a')`.
    '''
    import botocore
    if not partial_filename and key and not key.endswith('/'): key += '/'
    result_keys = []
    try:
        for page in s3_paginate('list_objects_v2', Bucket=bucket, Prefix=key, Delimiter='/'):
            for x in page.get('CommonPrefixes',[]):
                result_keys.append(x['Prefix'])
            for x in page.get('Contents',[]):
                if x['Key'] != key:
                    result_keys.append(x['Key'])
    except botocore.exceptions.ClientError as exc:
        raise Exception(f"Failed to get s3://{bucket}/{key}") from exc
    if not output_trailing_slash:
        result_keys = [k.rstrip('/') for k in result_keys]
    return [f'/mnt/s3/{bucket}/{k}' for k in result_keys]

def s3_paginate(s3_command:str, **kwargs):
    paginator = get_s3().get_paginator(s3_command)
    return paginator.paginate(**kwargs)

def s3_list_all_buckets() -> List[str]:
    return [bucket['Name'] for bucket in get_s3().list_buckets()['Buckets']]

def s3_get_head(bucket:str, key:str, bytes:int = 500) -> bytes:
    import botocore
    try:
        return get_s3().get_object(Bucket=bucket, Key=key, Range=f'bytes=0-{bytes}')['Body'].read()
    except botocore.exceptions.ClientError as exc:
        raise Exception(f"Failed to get s3://{bucket}/{key}") from exc

@functools.lru_cache(None)
def s3_get_object(bucket:str, key:str) -> dict:
    return get_s3().get_object(Bucket=bucket, Key=key)

@functools.lru_cache(None)
def get_s3() -> "boto3.client":
    import boto3
    return boto3.client('s3')

def smarter_open(path:str, mode:str = 'r'):
    import smart_open
    if path.startswith('/mnt/s3/'):
        path = 's3://' + remove_required_prefix(path, '/mnt/s3/')
    return smart_open.smart_open(path, mode)

def removeprefix(s:str, prefix:str) -> str:
    '''Backport of str.removeprefix(prefix)'''
    if s.startswith(prefix):
        return s[len(prefix):]
    return s

def remove_required_prefix(s:str, prefix:str) -> str:
    assert s.startswith(prefix)
    return s[len(prefix):]

def read_stdin_lines_and_switch_to_tty() -> List[str]:
    data = sys.stdin.readlines()
    #data = os.fdopen(os.dup(0), 'rb')
    os.dup2(os.open("/dev/tty", os.O_RDONLY), 0)
    return data



if __name__ == '__main__':
    run()
